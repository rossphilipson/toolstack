#!/bin/bash

# Use this NUMA configuration example file and the numaconfig utility to
# setup NUMA node affinity on the OpenXT platform. First copy this file to a
# location on the OpenXT system (e.g. /storage). This will allow customizing of
# the VM list and Dom0 settings below.
#
# Use of the numaconfig utility must be done using the sysadm_t role. From a
# terminal the role can be assumed using:
#
# $ nr
#
# If any of the VMs are NDVMs or the UIVM, the configuration files must be
# modified to allow the VMs configurations to be edited. This is an example on
# OpenXT with one NDVM. The same would be done for other NDVMs and the UIVM.
#
# Start by editing /config/vms/00000000-0000-0000-0000-000000000002.db and
# setting "modify-vm-settings" to "true". After this is done the dbd must be
# told to reload settings like this:
#
# $ kill -SIGHUP <pid-of-dbd>
#
# Xenmgr also has a "feature" that overwrites the NDVM and UIVM configs on
# every reboot. This has to be disabled to set affinity for these VMs. To
# do this:
#
# $ db-write /xenmgr/overwrite-ndvm-settings false
# $ db-write /xenmgr/overwrite-uivm-settings false
#
# Then setup the configurations below (Steps 1 and 2) for the system in
# question and run the script. Running the numaconfig script w/o any arguments
# will display the usage options.

# Step 1:
#
# The following list specifies what NUMA node to set the domain affinity to for
# each of the listed VMs. This is just a sample configuration. The values are
# of the form VMName:NUMANode:StubNUMANode. Any VMs not listed will not have
# any node affinity set. The StubNUMANode optionally allows setting of the node
# affinity for the associated stubdom.
#
# The primary reason is to allow a guest to only use memory local
# to the CPUs in the node that the VM has affinity to. The second way this is
# useful is for device pass-through where PCIe devices have proximity to one
# node or another. E.g. most systems will have thier PCH have proximity to
# NUMA node 0 (since by default if there is only one processor package it will
# be in node 0). So it makes sense to assign the Network VM to that node since
# it has the integrated NICs assigned to it (those devices being on the PCI(e)
# bus on the PCH).
VMLIST=(Win7VM1:1:1 Win7VM2:1:1 Win7VM3:1:1 Win7VM4:1:1 Network:0 uivm:0
WinTPC1:0:0 WinTPC2:0:0 WinTPC3:0:0 WinTPC4:0:0 WinTPC5:0:0
WinTPC6:1:0 WinTPC7:1:1 WinTPC8:1:1 WinTPC9:1:1 WinTPC10:1:1)

# Step 2:
#
# Dom0 is a special case because it needs its VCPU affinity set at boot time.
# This is done using e.g. "dom0_max_vcpus=4 dom0_vcpus_pin" on the Xen command
# line. This says give dom0 4 VCPUs and pin them to the first 4 CPUs. Though
# this is a bit limiting it is mostly fine since dom0 is the domain where all
# the devices on the PCH live that are not passed through. So in this example,
# CPUs 0 - 3 are in NUMA node 0 where the PCH is connected. Setting this value
# to zero will prevent the utility from changing dom0's default configuration.
DOM0=4
